{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.5.6 64-bit",
   "display_name": "Python 3.5.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9e12a6354b1e6324265ea93efbbe5e2a15bca608fa934c88fb5da6d5ec866c07"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Widzenie maszynowe\n",
    "## Laboratorium 4 - Segmentacja\n",
    "*Autor: Paweł Mendroch* - [Github](https://github.com/FrozenTear7/computer-vision-lab/tree/master/lab4)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Wykorzystując kod ze z [repozytorium](https://github.com/zhixuhao/unet), dokonuję segmentacji na przykładowych obrazach syntetycznych ze zbioru `hiragana`.\n",
    "\n",
    "Poniżej przedstawiam kod wykorzystany do treningu i testów."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Przy użyciu biblioteki OpenCV dla języka Python, generuję maski dla obrazów syntetycznych znajdujących się w `/train/image` i zapisuję je do folderu `/train/label` trzymając się konwencji ustalonej przez oryginalnego autora.\n",
    "\n",
    "Po odczytaniu obrazu część przezroczysta zostaje zamieniona w kolor czarny, który zamieniam na biały, a część obrazu, która stanowi faktyczny obraz (dłoń), koloruję na czarno, aby trzymać się kolorów narzuconych przez model.\n",
    "\n",
    "Kod przygotowujący całość obrazów znajduje się w pliku `prepareImages.py`.\n",
    "\n",
    "Przykładowe obrazy:\n",
    "\n",
    "![Image](./results/train_image_example.png \"Image\")\n",
    "![Label](./results/train_label_example.png \"Label\")\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "import os"
   ]
  },
  {
   "source": [
    "Wykonuję przygotowanie danych na podstawie `dataPrepare.ipynb`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 30 images belonging to 1 classes.\nFound 30 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGenerator = trainGenerator(20, 'data/hiragana/train', 'image', 'label', data_gen_args, save_to_dir=\"data/hiragana/train/aug\")\n",
    "\n",
    "num_batch = 3\n",
    "for i, batch in enumerate(myGenerator):\n",
    "    if(i >= num_batch):\n",
    "        break"
   ]
  },
  {
   "source": [
    "Poniżej wykonuję trening sieci przy pomocy sieci Unet, jak podane w `trainUnet.ipynb`.\n",
    "Metrykę dice coefficient dodałem w `model.py`.\n",
    "\n",
    "```\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true), -1) + K.sum(K.square(y_pred), -1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.0228 - dice_coef: 0.9772 - acc: 0.9466\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.02279, saving model to unet_hiragana.hdf5\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0036 - dice_coef: 0.9964 - acc: 0.9915\n",
      "\n",
      "Epoch 00002: loss improved from 0.02279 to 0.00358, saving model to unet_hiragana.hdf5\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0027 - dice_coef: 0.9973 - acc: 0.9935\n",
      "\n",
      "Epoch 00003: loss improved from 0.00358 to 0.00268, saving model to unet_hiragana.hdf5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b395a1be0>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(2, 'data/hiragana/train', 'image', 'label', data_gen_args, save_to_dir=None)\n",
    "\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint('unet_hiragana.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "model.fit_generator(myGene, steps_per_epoch=100, epochs=3, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30/30 [==============================] - 4s 140ms/step\n"
     ]
    }
   ],
   "source": [
    "testGene = testGenerator(\"data/hiragana/test\")\n",
    "model = unet()\n",
    "model.load_weights(\"unet_hiragana.hdf5\")\n",
    "results = model.predict_generator(testGene, 30, verbose=1)\n",
    "saveResult(\"data/hiragana/test\", results)"
   ]
  },
  {
   "source": [
    "Przeprowadziłem testy dla 3 zbiorów uczących o rozmiarach 30, 50 oraz 100 obrazów. Zbiór testowy zawierał 30 obrazów w każdym przypadku. Model uczyłem w 3 epokach po 100 kroków w każdej epoce i poniżej przedstawiam porównania wyników.\n",
    "\n",
    "- train: 30\n",
    "\n",
    "loss: 0.0026 - dice_coef: 0.9974 - acc: 0.9937\n",
    "\n",
    "- train: 50\n",
    "\n",
    "loss: 0.0026 - dice_coef: 0.9973 - acc: 0.9935\n",
    "\n",
    "- train: 100\n",
    "\n",
    "loss: 0.0026 - dice_coef: 0.9974 - acc: 0.9936\n",
    "\n",
    "Jak widać na powyższych wynikach, większy zbiór treningowy niekoniecznie daje lepsze wyniki, dla wszystkich trzech przypadków wyniki były praktycznie identyczne.\n",
    "\n",
    "Poniżej przedstawiam takie same testy przeprowadzone przy użyciu zdjęć RGB:\n",
    "\n",
    "- train: 30\n",
    "\n",
    "loss: 0.0027 - dice_coef: 0.9973 - acc: 0.9935\n",
    "\n",
    "- train: 50\n",
    "\n",
    "loss: 0.0026 - dice_coef: 0.9974 - acc: 0.9936\n",
    "\n",
    "- train: 100\n",
    "\n",
    "loss: 0.0026 - dice_coef: 0.9974 - acc: 0.9936\n",
    "\n",
    "Wersja kolorowa nie wprowadziła zmian w rezultatach, wyniki są porównywalne do wersji w skali szarości.\n",
    "\n",
    "Niestety nie udało mi się doprowadzić zapisu rezultatów do dobrze widocznych obrazów, co jest problemem powszechnym patrząc na zakładkę `Issues` na repo githubowym tego modelu.\n",
    "Najpierw obraz wahał się między całym białym lub czarnym rezultatem, po różnych odnalezionych zmianach udało mi się doprowadzić do zapisu w szarym kolorze, na którym widać lekką poświatę obrazu wynikowego.\n",
    "Obrazy wynikowe również zapisuję w formacie .tif, który nie jest obsługiwany przez Jupyter Notebook, przykładowy wynik znajduje się w folderze `results`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}